<!DOCTYPE html> <html> <head> <!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JMGDT16E1T"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-JMGDT16E1T'); </script> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> Anomaly detection with Deep AutoEncoders | </title> <meta name="description" content=" post 14 "> <meta name="keywords" content="AutoEncoders, Deep Learning, Pytorch, Encoder, Decoder, autoencoder, deep learning, implementation, neural network"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Facebook / Open Graph --> <meta property="og:type" content="article"> <meta property="article:author" content="Jerome Dinal Herath"> <meta property="article:section" content=""> <meta property="article:tag" content="AutoEncoders, Deep Learning, Pytorch, Encoder, Decoder, autoencoder, deep learning, implementation, neural network"> <meta property="article:published_time" content="2020-12-27 00:00:00 -0600"> <meta property="og:url" content="http://dinalherath.com/2020/autoencoder/"> <meta property="og:title" content=" Anomaly detection with Deep AutoEncoders | "> <meta property="og:image" content="http://dinalherath.com"> <meta property="og:description" content=" post 14 "> <meta property="og:site_name" content="Jerome Dinal Herath"> <meta property="og:locale" content="en_US"> <!-- Social: Twitter --> <meta name="twitter:card" content=""> <meta name="twitter:site" content=""> <meta name="twitter:title" content=" Anomaly detection with Deep AutoEncoders | "> <meta name="twitter:description" content=" post 14 "> <meta name="twitter:image:src" content="http://dinalherath.com"> <!-- Social: Google+ / Schema.org --> <meta itemprop="name" content=" Anomaly detection with Deep AutoEncoders | "> <meta itemprop="description" content=" post 14 "> <meta itemprop="image" content="http://dinalherath.com"> <!-- rel prev and next --> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="stylesheet" href="/assets/css/font-awesome.min.css"> <!-- Canonical link tag --> <link rel="canonical" href="http://dinalherath.com/2020/autoencoder/"> <link rel="alternate" type="application/rss+xml" title="" href="http://dinalherath.com/feed.xml"> <script type="text/javascript"> var disqus_shortname = ''; var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-99988883-1']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> </head> <body> <main class="wrapper"> <header class="site-header"> <nav class="nav"> <div class="container"> <h1 class="logo"><a href="/"><span></span></a></h1> <ul class="navbar"> <li><a href="/">Home</a></li> <!--<li><a href="/blog">Blog</a></li>--> <li><a href="/about me">About</a></li> <!-- <li><a href="/Resume.pdf">Resume</a></li> --> <!-- <li><a href="/resume">Resume</a></li>--> <!--<li><a href="/contact">Contact</a></li>--> </ul> </div> </nav> </header> <article class="post container" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <p class="post-meta"><time datetime="2020-12-27T00:00:00-06:00" itemprop="datePublished">Dec 27, 2020</time></p> <h1 class="post-title" itemprop="name headline">Anomaly detection with Deep AutoEncoders</h1> </header> <div class="post-content" itemprop="articleBody"> <p><img src="/material/2020/post_14/AE.jpeg?raw=true" alt="AE-image" width="540px" /> <br /></p> <p>Today I will be writing about another deep learning model named an <strong>AutoEncoder</strong>. As shown in the image above, an AutoEncoder model has two main components–1) an <code class="language-plaintext highlighter-rouge">Encoder</code> module and a 2) <code class="language-plaintext highlighter-rouge">Decoder</code> module. The Encoder is given some tensor input, named as <em>Y</em> in this case, and it learns some hidden representation of it usually called a <strong>hidden state</strong>. The Decoder in turn obtains this hidden state tensor and learns to reconstruct the original input <em>Y</em>. Given that, a trained model is able to recreate <em>Y</em> with minimum error, then it stands to reason that the hidden state is in fact some compact representation of the original input <em>Y</em>. Typically, it’s possible to obtain a hidden size that is much smaller with respect to the original tensor <em>Y</em>.</p> <p>As such, AutoEncoders have been used extensively to handle the <a href="https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e">curse of dimensionality</a> problem in Machine Learning. But in the post today, I will be focusing on the use of AutoEncoders as <a href="https://en.wikipedia.org/wiki/Anomaly_detection">anomaly detection</a> models while providing a skeleton code of a feed forwarding neural network based implementation using the <a href="https://pytorch.org">Pytorch</a> framework. I have divided the implementation into two sections. First an AutoEncoder Module which represents the construction of the neural network and then training and the anomaly detection process.</p> <h4 id="1-the-autoencoder-module">1. The AutoEncoder Module</h4> <p>Given that we have some data, an anomaly is a data point that is considered as a outlier sample. Outliers don’t really appear much in a given dataset, so from a supervised machine learning point of view outlier detection or anomaly detection can be a hard task. Because, typically supervised machine learning tasks work best when we have an equal distribution in the classes (i.e., benign class and the anomaly class). But as per the definition of anomalies, we won’t really see anomaly data points in a data set as much as we see normal behaviour. This being the case its possible to use AutoEncoder models in a semi-supervised manner in order to use the model for anomaly detection. This involves two steps:</p> <ol> <li>First the AutoEncoder model is trained on the benign class alone. Then a trained AutoEncoder will be able to accurately reconstruct any data sample from the <code class="language-plaintext highlighter-rouge">normal</code> class. The reconstruction error or loss of the model can be used as an anomaly score. Since the reconstruction of a benign class sample must be accurate, then the anomaly score would be low.</li> <li>During deployment, whenever the AutoEncoder encounters an anomaly sample, it would not be able to recreate the anomaly sample accurately. This would result in a large reconstruction error for this specific data sample. Whenever this anomaly score is high, it’s likely that the encountered data sample is an anomaly. When its low, then its most likely a normal data point.</li> </ol> <p>The following <code class="language-plaintext highlighter-rouge">AutoEncoderModule</code> python class gives an implementation using feed forwarding neural networks as the basis of the Encoder and the Decoder. Notice, that in the <em>init()</em> function I have defined two sequentially concatenated layers. The first Encoder component is a neural network that decreases in size starting from the original input dimension down to a hidden state size. Similarly, the Decoder is a neural network that increases in size starting from the hidden state size up to the dimensions of the original input. This is a typical implementation of an AutoEncoder-Module, depending on the type of data you are working on it is possible to use <code class="language-plaintext highlighter-rouge">LSTM/GRU</code> for the Encoder/Decoder if its time series data or a <code class="language-plaintext highlighter-rouge">CNN/GNN</code> if its image data or graphs. The <em>forward()</em> function gives both the reconstructed input and the hidden state.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span> 
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">AutoEncoderModule</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">""" the pytorch Neural Network module of the AutoEncoder """</span>
    <span class="s">""" The current implementation uses a feed forwarding neural network """</span>
    <span class="s">""" as the encoder and the decoder """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="s">""" init function"""</span>
        <span class="s">""" @param input_dim: the input dimension of the tensor """</span>
        <span class="s">""" @param hidden_size: the dimension of the hidden size """</span>
        <span class="s">""" @param device: cpu or gpu device to run in """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AutoEncoderModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="s">""" define the encoder/ decoder layer steps """</span>
        <span class="n">dec_steps</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)),</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">input_dim</span><span class="p">))</span>
        <span class="n">dec_setup</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">dec_steps</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]])</span>
        <span class="n">enc_setup</span> <span class="o">=</span> <span class="n">dec_setup</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="s">""" encoder definition """</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)]</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">enc_setup</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]).</span><span class="n">flatten</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_encoder</span><span class="p">)</span>

        <span class="s">""" decoder definition """</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)]</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">dec_setup</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]).</span><span class="n">flatten</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_decoder</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ts_batch</span><span class="p">):</span>
        <span class="s">""" forward pass """</span>
        <span class="s">""" @param ts_batch: the batch of input tensors """</span>
        <span class="s">""" @returns reconstructed_sequence and the hidden_state (enc) """</span>
        <span class="n">flattened_sequence</span> <span class="o">=</span> <span class="n">ts_batch</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">ts_batch</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">enc</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">flattened_sequence</span><span class="p">.</span><span class="nb">float</span><span class="p">())</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
        <span class="n">reconstructed_sequence</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">ts_batch</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">reconstructed_sequence</span><span class="p">,</span> <span class="n">enc</span>

    <span class="k">def</span> <span class="nf">to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="s">""" helper code to send to device """</span>
        <span class="n">module</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span>
</code></pre></div></div> <h4 id="2-training-and-anomaly-detection">2. Training and Anomaly Detection</h4> <p>In the code below, I have used an instance of the above <code class="language-plaintext highlighter-rouge">AutoEncoderModule</code> and defined the training and anomaly detection tasks in the functions <em>fit()</em> and <em>predict()</em>. The initialization of the AutoEncoder is similar to a typical deep learning model with the parameters of batch size, learning rate, epochs to train and the device. The model specific parameters are the hidden size and the input dimension.</p> <p>Notice, that in the <em>init</em> function, I have two variables for <code class="language-plaintext highlighter-rouge">max_err</code> and <code class="language-plaintext highlighter-rouge">min_err</code>. In practice, I usually prefer to normalize the reconstruction error within a specific range(i.e., [0,1]). This can be done by normalizing the reconstruction error within the minimum and maximum error values obtained during its training phase. Then during anomaly detection we can consider a sample to be an anomaly if the normalized reconstructed error (\(\beta\)) is greater than a given threshold (\(\theta\)).</p> <p>As such, the <em>fit()</em> function does two tasks. First it trains the model for a given number of epochs on the training data. Afterwards it does one forward pass on the training data and identifies the minimum and maximum reconstruction loss. In this code example I have used the <code class="language-plaintext highlighter-rouge">MSELoss</code> for the training iterations and the <code class="language-plaintext highlighter-rouge">L1Loss</code> for anomaly detection. It’s possible to use other loss functions along with more complicated procedures to obtain the anomaly scores depending on the complexity of the application domain. The <em>predict()</em> function is quite straightforward. It basically does a forward pass on the data and computes anomaly scores. Unlike in the training phase we do not need to calculate the gradients, so using <em>with torch.no_grad()</em> usually saves time during this phase.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AutoEncoder</span><span class="p">:</span>
    <span class="s">""" AutoEncoder model designed for anomaly detection """</span>
    <span class="s">""" uses the 'AutoEncoderModule' class """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">run_in_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="s">""" init function"""</span>
        <span class="s">""" @param input_dim: the input dimensions """</span>
        <span class="s">""" @param hidden_size: hidden state size """</span>
        <span class="s">""" @param batch_size: batch size for single forward pass """</span>
        <span class="s">""" @param learning_rate: learning rate to train model """</span>
        <span class="s">""" @param num_epochs: the #iterations to train """</span>
        <span class="s">""" @param run_in_gpu: True if GPU is used """</span>
        <span class="s">""" AE model parameters """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">run_in_gpu</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoEncoderModule</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="s">""" setting the training parameters """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span>
        
        <span class="s">""" anomaly score normalizing constants """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_err</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_err</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">""" training the AutoEncoder model """</span>
        <span class="s">""" @param X: the training data """</span>
        <span class="s">""" (1) the training process """</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">ts_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">_</span>  <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts_batch</span><span class="p">))</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts_batch</span><span class="p">.</span><span class="nb">float</span><span class="p">()))</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="s">""" (2) get error values to normalize anomaly score (optional) """</span>
        <span class="n">train_anomaly_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_anomaly_loader</span><span class="p">):</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts</span><span class="p">))</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts</span><span class="p">.</span><span class="nb">float</span><span class="p">()))</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">error</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>   
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_err</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_err</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">""" complete pass for obtaining the anomaly score """</span>
        <span class="s">""" @param X: the test data """</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>     
                <span class="s">""" does one forward pass """</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts</span><span class="p">))</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_var</span><span class="p">(</span><span class="n">ts</span><span class="p">.</span><span class="nb">float</span><span class="p">()))</span>
                <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                <span class="s">""" gets the anomaly score, normalized """</span>
                <span class="n">anomaly_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_err</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_err</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_err</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">anomaly_scores</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">data</span>

    <span class="k">def</span> <span class="nf">to_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div> <p>There you go, a speedy run through on how to code up an AutoEncoder model using Pytorch. Depending on your need its possible to use much more sophisticated neural network architectures and loss/error calculation metrics in AutoEncoders.</p> <h5 id="so-until-next-time">So until next time,</h5> <h5 id="cheers">Cheers!</h5> <p><strong>Next: <a href="/2021/lam/">Fooling AI-based System Log Anomaly Detection</a></strong> <br /> <strong>Prev: <a href="/2020/ramp/">RAMP: Real-Time Aggregated Matrix Profile</a></strong></p> <aside class="share"> <p>If you liked this article and think others should read it, please <a href="http://twitter.com/share?text=Anomaly detection with Deep AutoEncoders&amp;url=http://dinalherath.com/2020/autoencoder/&amp;via=" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">share it on Twitter <i class="fa fa-twitter" aria-hidden="true" style="color:#00aced"></i></a> or <a href='https://www.facebook.com/sharer/sharer.php?u='+url; onclick="window.open(this.href, 'sharer', 'toolbar=0,status=0,width=550,height=235');return false;">Facebook <i class="fa fa-facebook-square" aria-hidden="true" style="color:#3b5998"></i></a>.</p> </aside> </div> </article> <!-- --> <footer class="site-footer"> <!--<link rel="stylesheet" href="/assets/icons/academicons-1.8.0/css/academicons.css"/>--> <div class="container"> <small class="block"><!--&lt;/&gt; <a href="http://github.com/heiswayi/thinkspace" title="a minimalist Jekyll theme for technical writing">Thinkspace theme</a> by <a href="http://heiswayi.github.io">Heiswayi Nrird</a>. |--><i class="fa fa-copyright"></i> 2017-2025 Jerome Dinal Herath</small> <!-- <a href="mailto:dinal.bing@gmail.com"><i class="fa fa-envelope" style="font-size:21px;color:#F15B3D;"></i></a> --> <!-- <a href="https://www.facebook.com/dinalHerath" target="_blank"><i class="fa fa-facebook-square" style="font-size:22px;color:#F15B3D"></i></a> --> <a href="https://github.com/dherath" target="blank"><i class="fa fa-github-square" style="font-size:22px;color:#F15B3D"></i></a> <a href="https://www.linkedin.com/in/jerome-dinal-herath-bba3b0148/" target="blank"><i class="fa fa-linkedin-square" style="font-size:22px;color:#F15B3D"></i></a> <a href="https://scholar.google.com/citations?user=vNtiUMwAAAAJ&hl=en" target="blank"><i class="fa fa-google" style="font-size:22px;color:#F15B3D"></i></a> <!-- <a href="/feed.xml" target="_blank"><i class="fa fa-rss-square" style="font-size:22px;color:#F15B3D"></i></a> --> <!--<a href="https://www.researchgate.net/profile/Jerome_Dinal_Herath" target="blank"> <i style="font-size:20px;color:#F15B3D;font-weight:bold;"> RG </i></a> --> </div> </footer> </main> </body> </html>
