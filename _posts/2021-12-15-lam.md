---
layout: post
title: "Fooling AI-based System Log Anomaly Detection"
comments: false
description: "post 15"
keywords: "distributed, system, logs, adversarial, attack, LAM, evasion, log, anomaly, mask, Log-Anomaly-Mask, deep learning, machine learning, anomaly detection, DeepLog, AutoEncoder"
---

Hi All!! In today's post I will again be writing about one of my own research work I completed as part of my PhD. The entire premise of this work revolves around a popular sub-field in Machine Learning called, [Adversarial Machine Learning](). It's no surprise that Machine Learning and Deep Learning models are capable of accomplishing a varying range of real-world complex tasks, from image recognition, time series prediction and playing games like GO. However, in recent years researchers have shown that AI can be, in fact **fooled**. 

![attack-image]({{site.url}}/material/2021/post_15/adv_attack.jpeg){:width="640px"}
<br>

The image above is a prime example for that. In the seminal work by [Goodfellow et al](), it is shown that a Deep Learning model trained to recognize an image can be fooled by making extremely small changes to the original image. In this example, as you can see the model classifies an image of a Panda as a Gibbon. From that point on wards, many researchers have identified ways to [exploit weaknesses in AI models](), both Machine Learning and Deep Learning based. 

This growing concern is specifically important for AI used in the cybersecurity domain, because if models can be fooled then it would be possible for an attacker to engineer methods in evading automated threat detection. Thereby wrecking havoc without a care of being caught.

So today I will be talking about our own contribution towards Adversarial AI in the application of anomaly detection from distributed system logs. In this work, we put on the hat of an attacker, where our goal was to find an automated method that can evade anomaly detection by even state-of-the-art system log anomaly detection models such as [DeepLog](). I won't be gong into minute details about our attack method, instead I will provide an overview of the challenges and the design of our attack. For algorithmic details, please refer to our [paper]() and the [code]().

#### 1. Anomaly Detection from Distributed System Logs

![anomaly-detection]({{site.url}}/material/2021/post_15/log_anomaly_detection_example.jpeg){:width="600px"}
<br>


#### 2. Our Attack: Log-Anomaly-Mask (LAM)

![lam]({{site.url}}/material/2021/post_15/LAM_image.jpg){:width="600px"}
<br>


![attack-example]({{site.url}}/material/2021/post_15/log_attack_example.jpeg){:width="660px"}
<br>



+ intro
    - adversarial AI
    - system log anomaly detection & models that work
    - the adversarial attack
        - the attack method
        - Reinforcement Learning as an Attack
            : write about main components and why we chose them
            : why reinforcement learning -> what kind of RL? why?

+ images
    - system log anomaly detection
    - attack example (logs) 
    - attack example (images)
    - distributed system log anomaly detection
    - model


##### So until next time,
##### Cheers!

**Prev: [RAMP: Real-Time Aggregated Matrix Profile]({{site.url}}/2020/ramp/)**